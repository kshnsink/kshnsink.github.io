<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Resource Management in Kubernetes Cluster on Kishan&#39;s World</title>
    <link>http://kshnsink.com/series/resource-management-in-kubernetes-cluster/</link>
    <description>Recent content in Resource Management in Kubernetes Cluster on Kishan&#39;s World</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 May 2023 09:55:51 +0530</lastBuildDate><atom:link href="http://kshnsink.com/series/resource-management-in-kubernetes-cluster/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Resource Mangement in Kubernetes Cluster: Isolation Using Namespaces</title>
      <link>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterisolation-using-namespaces/</link>
      <pubDate>Mon, 29 May 2023 09:55:51 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterisolation-using-namespaces/</guid>
      <description>Understanding Isolation using Namespaces in Kubernetes</description>
      <content:encoded><![CDATA[<h1 id="isolation-using-namespaces">Isolation using Namespaces</h1>
<p>In Kubernetes, namespaces are a way to create virtual clusters within a physical cluster, providing resource isolation and segregation. Think of it like kernel namespace which is a feature to isolate resources from each other. Analogous to a physical world example, think of it like a store in a shopping mall which has its own physical piece of land, electricity bill, water bill, interior design in a shared shopping complex. The shopping complex is our cluster and the store is a namespace within the cluster. A namespace is a logical boundary that allows you to divide cluster resources and create separate environments for different applications, teams, or projects.</p>
<h2 id="how-namespaces-create-isolation">How Namespaces create Isolation?</h2>
<p>Resource isolation using namespaces helps prevent interference between different workloads running on the same Kubernetes cluster. Let’s see how it can be achieved:</p>
<h3 id="namespaced-scope">Namespaced Scope</h3>
<p>Kubernetes resources such as pods, services, deployments, replica sets belong to a specific namespace. By default, resources are created in the <code>default</code> namespace if no namespace is specified. Each namespace provides an isolated environments where resources can be created and managed. This means that you could have a service called <code>HelloWorld</code> in a <code>production</code> namespace and a different service called <code>HelloWorld</code> in the <code>test</code> namespace and there won’t be any conflict.</p>
<blockquote>
<p>Namespaces are logically isolated from one another but they can still communicate with services in another namespace.</p>
</blockquote>
<p>Service DNS names folllow the <code>SERVICE.NAMESPACE.svc.cluster.local</code> pattern. Kubernetes DNS service directory can easily locate any service by its name by using the expanded form of DNS addressing. To access HelloWorld service in the production namespace, simply use <code>helloworld.production</code></p>
<h3 id="resource-allocation">Resource Allocation</h3>
<p>Namespaces allow you to allocate and limit resources separately for different workloads. For example, you can allocate specific amount of CPU and memory resources to a namespace and the resources within the namespace will be constrained by the specified limits. This is done with the help of ResourceQuotas and LimitRanges which are the objects used to control resource usage by the cluster administrator.</p>
<h4 id="resourcequota">ResourceQuota</h4>
<p>The ResourceQuota is the total allocated resources for a particular namespace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ResourceQuota</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">quota</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">hard</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">limits.cpu</span>: <span style="color:#e6db74">&#34;50&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">limits.memory</span>: <span style="color:#ae81ff">100Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests.cpu</span>: <span style="color:#e6db74">&#34;25&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests.memory</span>: <span style="color:#ae81ff">50Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">pods</span>: <span style="color:#e6db74">&#34;50&#34;</span>
</span></span></code></pre></div><h4 id="limitrange">LimitRange</h4>
<p>LimitRange is used for managing constraints of resource allocation at a pod and container level within the namespace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;v1&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#e6db74">&#34;LimitRange&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;resource-limits&#34;</span> 
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>    -
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;Pod&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">max</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;2&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;1Gi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">min</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;200m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;6Mi&#34;</span> 
</span></span><span style="display:flex;"><span>    -
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;Container&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">max</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;2&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;1Gi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">min</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;100m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;4Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">default</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;300m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;200Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">defaultRequest</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;200m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;100Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">maxLimitRequestRatio</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;10&#34;</span>
</span></span></code></pre></div><p><img loading="lazy" src="/img/kubernetes_resource_quota_limit_range.png" alt=" "  />
</p>
<h3 id="access-control">Access Control</h3>
<p>Namespaces provide a mechanism for access control and permissions. You can define role-based access control(<strong>RBAC</strong>) policies to grant different levels of access to users or groups for resources within a namespace. This enables fine-grained control over who can access and modify(update) resources within a specific cluster and also within a namespace, enhancing security and isolation. In RBAC there are three components</p>
<h4 id="users-and-groups">Users and Groups</h4>
<p>Users are individuals who need access to the cluster, while groups are collections of users with similar roles</p>
<h4 id="roles">Roles</h4>
<p>A role is a set of rules that define a set of permissions within a specific namespace. Roles can be created at the namespace level using <code>Role</code>object and are used to grant access to resources within the namespace. It is also possible to define the role which has access across the cluster using <code>ClusterRole</code> object</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>] <span style="color:#75715e"># &#34;&#34; indicates the core API group</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;pods&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># &#34;namespace&#34; omitted since ClusterRoles are not namespaced</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># at the HTTP level, the name of the resource for accessing Secret</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># objects is &#34;secrets&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;secrets&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span></code></pre></div><h4 id="rolebindings-and-clusterrolebindings">RoleBindings and ClusterRoleBindings</h4>
<p><strong>RoleBindings</strong> associate a role with a user or a group, granting the defined permissions to the user/group within a specific namespace.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This role binding allows &#34;jane&#34; to read pods in the &#34;default&#34; namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You need to already have a Role named &#34;pod-reader&#34; in that namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-pods</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You can specify more than one &#34;subject&#34;</span>
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">jane</span> <span style="color:#75715e"># &#34;name&#34; is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># &#34;roleRef&#34; specifies the binding to a Role / ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span> <span style="color:#75715e">#this must be Role or ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span> <span style="color:#75715e"># this must match the name of the Role or ClusterRole you wish to bind to</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p><strong>ClusterRoleBindings</strong> on the other hand, associate a role with a user/group across the entire cluster, granting permissions across all namespaces</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This role binding allows &#34;dave&#34; to read secrets in the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You need to already have a ClusterRole named &#34;secret-reader&#34;.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-secrets</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># The namespace of the RoleBinding determines where the permissions are granted.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># This only grants permissions within the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">development</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dave</span> <span style="color:#75715e"># Name is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p>Implementing RBAC enforces strong access controls ensuring that only authorized users/groups have appropriate level of access to cluster resources.</p>
<p>By leveraging namespaces, Kubernetes provides a powerful mechanism for isolating and managing resources within a cluster. It allows you to run multiple applications or projects on the same cluster without interference or noisy neighbour problem and provides control over resource allocation, access control, and resource quotas at the namespace level.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Resource Mangement in Kubernetes Cluster: Understanding Resources</title>
      <link>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterunderstanding-resources/</link>
      <pubDate>Thu, 11 May 2023 03:52:06 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterunderstanding-resources/</guid>
      <description>Understanding Kubernetes resources and Quality of Service classes</description>
      <content:encoded><![CDATA[<p>Kubernetes is a container orchestration platform that allows you to manage and automate the deployment, scaling and management of containerised application. In this post we will look into the resource management in Kubernetes which involves allocating resources such as CPU, memory to the containers running in a cluster</p>
<h1 id="understanding-resource">Understanding Resource</h1>
<p>At the heart of a Kubernetes cluster  is a resource abstraction called <strong>pods</strong> which groups containers together and manage their resources as a unit.  ****Each pod is assigned a certain amount of CPU and memory resources, which are used by the containers running within the pod. Pods run on a node in the cluster and the node size in terms of memory and CPU in a cluster is definite. Only certain number of pods can be run in a cluster and it is a duty of a Kubernetes cluster admin to assign the resources to the pod optimally to get the best utilization and at the same time ensuring that there’s enough room to deal with increasing load and failures.</p>
<p>Kubernetes scheduler decides where to place a pod in the cluster, basically which node has spare resource. In order to schedule pods effectively, the scheduler must know the resource requirements for each pod. This is where Kubernetes resource requests and limits kicks in</p>
<h2 id="container-resource">Container Resource</h2>
<h3 id="requests-and-limitshttpskubernetesiodocsconceptsconfigurationmanage-resources-containersrequests-and-limits"><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Requests and Limits</a></h3>
<p>Kubernetes resource configuration consists of two components: <code>requests</code> and <code>limits</code>.</p>
<ul>
<li>request specifies the minimum amount of a request that a pod needs to run.</li>
<li>limit specifies the maximum amount of resource that a pod is allowed to use.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;200Mi&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;400m&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;400Mi&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;800m&#34;</span>
</span></span></code></pre></div><p>Setting resource requests and limits allows to accommodate spiky Pods in a cluster. Resource limits are a hard boundary for a pod. A pod that tries to use more than its allocated CPU limit which is a <em>compressible resource</em> will be throttled thus impacting performance and if it tries to use more than the allowed memory limit which is an incompressible resource, the pod gets terminated with OOM error. Scheduler tries to schedule the pod in any other nodes in the cluster if there’s enough memory in the node. Kubernetes allows resources to be overcommitted which means the sum of all the resource limits of containers on a cluster(node) can exceed the total resources.</p>
<p>Despite all the efforts put upfront in defining the resource requirements, what happens if the containers are using more resources than allocated. Kubernetes uses <code>Quality of Service (QoS)</code> classes to make a decision about evicting pods in resource crunch situations.</p>
<h2 id="quality-of-serviceqos">Quality of Service(QoS)</h2>
<p>Kubernetes has 3 QoS classes <em>Guaranteed</em>, <em>Burstable</em>, or <em>BestEffort</em> defined. This cannot be assigned directly by you, rather Kubernetes does it for you on the basis of resources defined in the Pod manifest.</p>
<h3 id="guaranteed">Guaranteed</h3>
<p>When containers Limits(CPU and Memory) matches requests(CPU and memory). This essentially means that control plane kills this Pod if it exceeds the specified limits.</p>
<p><img loading="lazy" src="/img/guaranteed_qos_class.jpg" alt=" "  />
</p>
<blockquote>
<p>Note: If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">kubectl get po qos-pod-guaranteed -o yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">status</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">qosClass</span>: <span style="color:#ae81ff">Guaranteed </span>
</span></span></code></pre></div><h3 id="burstable">Burstable</h3>
<p>When containers Limits is higher than Requests. Kubernetes allows burstable pods upto their limit if capacity is available in the node. If the pod uses more resources than the request and there’s not enough resource available in the node, the pod gets terminated.</p>
<p><img loading="lazy" src="/img/burstable_qos_class.jpg" alt=" "  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">kubectl get po qos-pod-burstable -o yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">400m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">800Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">status</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">qosClass</span>: <span style="color:#ae81ff">Burstable </span>
</span></span></code></pre></div><h3 id="besteffort">BestEffort</h3>
<p>When containers doesn’t specify any resource request or limits. In this situation pods are allowed to use whatever resource is available on the node but it will be the first one to be killed when the cluster needs to make room for higher(Burstable/Guaranteed) QoS pods.</p>
<p><img loading="lazy" src="/img/besteffort_qos_class.jpg" alt=" "  />
</p>
<h2 id="what-should-you-do">What should you do?</h2>
<p>In the first instance it seems that Guaranteed is the best QoS class to set for the pods. But remember resource(memory/CPU) has some cost 💰. Let us try and understand how this can be approached. Considering we have a definite resource at hand and if we want to set request and limits equal(Guaranteed QoS class) we have to either increase requests or lower the limits. In case of increasing the resource, we might be blocking too much of resource unnecessarily and some other Pod which could have been scheduled in the node is not getting the resource. If we lower the Limit, the pod might throttle during the peak/spike hours. This seems like a similar territory when VMs are used.</p>
<blockquote>
<p><em>Scheduling of a Pod is based on requests and not limits</em></p>
</blockquote>
<p>The place where Kubernetes is different from the conventional resource allocation is when we use Burstable pods. Here, the amount of resource blocked by the pod is lower than the amount of resource pod needs during surge hours.</p>
<p>Pods with QoS class as <em>BestEffort</em> gets the <strong>lowest priority.</strong> If resource is not specified, Kubernetes scheduler will place such pods on any of the node which has available resource.</p>
<p>During the eviction, Kubelet selects the Pods to evict in order of QoS class. Pods classified as BestEffort will be first evicted followed by Burstable and finally Guaranteed.</p>
<blockquote>
<p><em>Always specify resource requests and limits. This helps Kubernetes schedule and manage the pods properly. For critical pods or stateful sets, prefer Guaranteed class and Burstable for the less critical ones</em></p>
</blockquote>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
