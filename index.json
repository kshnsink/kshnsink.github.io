[{"content":"In this post, we look at how to overcome slow queries by analysing them with Explain and Analyze, and using indexes to modify and enhance the query timings.\nPostgres supports different kinds of indexing on the table for querying faster.\nMultiple column indexes A multi-column B-Tree index can be used with query conditions that involve any subset of the index\u0026rsquo;s columns. This index is most efficient when there are constraints on the leading (leftmost) columns. The exact rule is that equality constraints on leading columns, plus any inequality constraints on the first column that does not have an equality constraint, will be used to limit the portion of the index that is scanned.\nCover-Index An index containing all the columns needed for a query, which is there in the select statement.\nUnique Index A unique index is an index used to enforce uniqueness of a column\u0026rsquo;s value or the uniqueness of a combined value of more than one column.\nOne of the most misunderstood concepts around indexing is to understand where to use a primary key, unique constraint, or unique index. Let\u0026rsquo;s understand this using a problem:\nProblem statement We require maximum performance with no duplicate data. Which is the better approach? Primary key, unique constraint or unique index?\nSolution Note: Multiple null values are not equal, so they are not considered as a duplicate record.\nPostgres automatically creates a unique index in the table when a primary key and unique constraint is defined in the table. Creating unique constraint then would be redundant, and unnecessarily creating indexes degrades the performance of Postgres. According to suggestions by the Postgres product team, create a unique constraint on the table and then there is no need to create a unique index on those columns. Postgres creates an index for the defined primary key itself When we create a unique constraint, Postgres automatically creates an index behind the scene. However, there are cases where even indexing can\u0026rsquo;t improve performance. One such case is when we do case-insensitive searches. Let\u0026rsquo;s understand the difference between the query cost in case of case sensitive and case insensitive search in our scheme table. Given we have an index on the column scheme_name.\nEXPLAIN ANALYSE SELECT * FROM schemes where scheme_name = 'weekend_scheme'\nQUERY PLAN | Index scan using idx_scheme_name on schemes (cost=0.28..8.29 rows=1 width=384) Planning Time: 0.155 ms Execution Time: 0.063ms\nEXPLAIN ANALYSE SELECT * FROM schemes where lower(scheme_name) = 'weekend_scheme'\nQUERY PLAN | Seq Scan on schemes (cost=0.00..69.00 rows=5 width=384) Filter: (lower((scheme_name) :: text) = \u0026lsquo;weekend_scheme\u0026rsquo; :: text) Rows removed by filter: 999 Planning Time: 0.119 ms Execution Time: 0.721ms\nEven though we have an index created at scheme_name, the function lower degrades the performance as it does an additional effort of converting all the values of scheme_table to lower case.\nCases when an index is not used (although it is defined).\nLIKE '%scheme' will never use an index, but LIKE 'scheme%' can possibly use the index. The upper/lower case function used in where clause. So whenever we want to use a function in our where clause, we could create the index in the following way to optimise the query. CREATE INDEX idx_scheme_name ON schemes (lower(scheme_name))\nEXPLAIN ANALYSE SELECT * FROM schemes where lower(scheme_name) = 'weekend_scheme'\nQUERY PLAN | Bitmap heap scan on schemes ((cost=4.32..19.83 rows=5 width=384)) Recheck cond: (lower ((scheme_name) :: text) = \u0026lsquo;weekend_scheme\u0026rsquo; :: text) Heap Blocks: exact=1 Bitmap scan on schemes ((cost=0.00..4.32 rows=5 width=0)) Index cond: (lower ((scheme_name) :: text) = \u0026lsquo;weekend_scheme\u0026rsquo; :: text) Planning Time: 1.784 ms Execution Time: 0.079 ms\nPartial Index Postgres supports an index over a subset of the rows of a table (known as a partial index). It\u0026rsquo;s often the best way to index our data if we want to repeatedly analyse rows that match a given WHERE clause. Let us see how we can enhance the performance of Postgres using partial indexing.\nProblem statement We want to return all the schemes which are supposed to run before 11:00 am.\nSolution EXPLAIN ANALYSE SELECT * FROM schemes WHERE start_time \u0026lt; '10:00:00'\nQUERY PLAN | Seq Scan on schemes (cost=0.00..66.50 rows=9 width=23) Filter: (start_time \u0026lt; \u0026lsquo;10:00:00\u0026rsquo;) Rows removed by filter: 991 Planning Time: 0.082 ms Execution Time: 0.226 ms\nWe can create an index on start_time column but assuming we have a huge database, this may not be optimal for insert, update and delete. So we create an index with a condition. This kind of indexing is used when we know what we need from our select queries. Say we do a heavy read on all the schemes which are started before 10:00:00 and not much when started later.\nCREATE INDEX idx_scheme_name ON schemes start_time WHERE start_time \u0026lt; '11:00:00'\nEXPLAIN ANALYSE SELECT * FROM schemes WHERE start_time \u0026lt; '10:00:00'\nQUERY PLAN | Bitmap heap scan on schemes ((cost=4.21..29.30 rows=9 width=23)) Recheck cond: (start_time \u0026lt; \u0026lsquo;10:00:00\u0026rsquo;) Heap Blocks: exact=8 Bitmap index scan on schemes ((cost=0.00..4.21 rows=9 width=0) Index cond: (start_time \u0026lt; \u0026lsquo;10:00:00\u0026rsquo;) Planning Time: 1.729 ms Execution Time: 0.075 ms\nThis reduces the execution time from 0.226to 0.075. Let\u0026rsquo;s validate that we have not indexed all the schemes where start_time is after 11:00 AM.\nEXPLAIN ANALYSE SELECT * FROM schemes WHERE start_time \u0026gt;'12:00:00'\nQUERY PLAN | Seq Scan on schemes (cost=0.00..66.50 rows=6 width=23) Filter: (start_time \u0026lt; \u0026lsquo;12:00:00\u0026rsquo;) Rows removed by filter: 993 Planning Time: 0.101 ms Execution Time: 0.228ms\nThis proves that partial data from schemes table is indexed and the rest of the data is not indexed. Our index size is very small and easy to maintain, helping in the maintaining task of reindexing.\nQuery plan on JOINS The optimizer needs to pick the correct join algorithm when there are multiple tables to be joined in the select statement. Postgres uses 3 different kinds of join algorithm based on the type of join we are using.\nNested Loop: Here, the planner can use either sequential scan or index scan for each of the elements in the first table. The planner uses a sequential scan when the second table is small. The basic logic of choosing between a sequential scan and index scan applies here too. Hash Join: In this algorithm, the planner creates a hash table of the smaller table on the join key. The larger table is then scanned, searching the hash table for the rows which meet the join condition. This requires a lot of memory to store the hash table in the first place. Merge Join: This is similar to merge sort algorithm. Here the planner sorts both the tables to be joined on the join attribute. The tables are then scanned in parallel to find the matching values.\nEXPLAIN SELECT schemes.rules FROM scheme_rules JOIN schemes ON (scheme_rules.scheme_id = schemes.id ) where scheme_name = 'weekend_scheme';\nDownsides of indexes in production environments Finding unused indexes In a large production environment, finding unused indexes is advisable, because indexes eat memory. Postgres wiki page details how we can find index summary, duplicate indexes, and index size.\nCREATE/DROP index vs CREATE/DROP index concurrently Creating and dropping an index in a large database can take hours or even days and the CREATE INDEX command blocks all the writes on a table (it doesn\u0026rsquo;t block the reads, but this is still not ideal).\nHowever, an index created concurrently with CREATE INDEX CONCURRENT will not acquire locks against writes. When creating index concurrently, Postgres first scans the table to build indexes and runs the index once again for the things to be added since the first pass.\nCreating an index concurrently also has a downside though. If something goes wrong during the process, it does not roll back, and leaves an invalid index behind. Invalid indexes can be found using the following query.\nSELECT * FROM pg_class, pg_index WHERE pg_index.indisvalid = false AND pg_index.indexrelid = pg_class.oid;\nRebuilding indexes REINDEX rebuilds an index using the data stored in the index table, replacing the old copy of the index. If we suspect corruption of an index on a table, we can simply rebuild that index, or all indexes on the table, using REINDEX INDEX or REINDEX TABLE\nREINDEX is similar to a drop and recreate of the index in that the index contents are rebuilt from scratch. However, locking considerations are rather different. REINDEX locks out writes but not reads of the index\u0026rsquo;s parent table. It also takes an exclusive lock on the specific index being processed, which will block reads that attempt to use that index.\nAnother option is to drop index concurrently and create again concurrently.\nConclusion This post aimed to provide an overview of how Postgres queries the database. By understanding query plans better and carefully taking measures (mostly through indexes), we can get the best performance out of the Postgres database.\nFurther Reading Index locking consideration Locking indexes Routine reindexing Examining index usage Monitoring stats ","permalink":"http://kshnsink.com/posts/postgres-performance-tuning-manualindexes/","summary":"In this post, we look at how to overcome slow queries by analysing them with Explain and Analyze, and using indexes to modify and enhance the query timings.\nPostgres supports different kinds of indexing on the table for querying faster.\nMultiple column indexes A multi-column B-Tree index can be used with query conditions that involve any subset of the index\u0026rsquo;s columns. This index is most efficient when there are constraints on the leading (leftmost) columns.","title":"Postgres Performance Tuning Manual: Indexes"},{"content":"Postgres is one of the most widely used open source databases in the world. At GOJEK, a lot of our products depend on Postgres as well. A lot of major companies use Postgres as their main database however when you\u0026rsquo;re building and operating at scale, the volume of data passing through the pipelines can slow down the most efficient systems.\nTo optimise things in order to enhace performace, we can target the brain of the database whhich is optimizer. Optimizer interprets queries and determines the fastest method of execution. A single query optimization technique can increase database performance drastically.\nThis post outlines how to analyse Postgres performance using tools such as EXPLAIN and ANALYZE (which Postgres provides).\nMeet EXPLAIN \u0026amp; ANALYZE EXPLAIN gives an exact breakdown of a query. The execution plan is based on the statistics about the table, and it identifies the most efficient path to the data. This takes different database paradigms (such as indexes) into consideration. EXPLAIN only guesses a plan that it thinks it will execute. This is where ANALYZE comes into the picture. ANALYZE basically runs a query to find the processing time to execute a query.\nTo quickly summarise, EXPLAIN and ANALYZE commands help improve database performance in Postgres by:\na. Displaying the execution plan that the PostgreSQL planner generates for the supplied statement.\nb. Actually running the command to show the run time.\nFinding the framework Let\u0026rsquo;s consider we have a table named schemes.\nEXPLAIN SELECT * FROM schemes;\nQUERY PLAN | Seq Scan on schemes (cost=0.00..64 rows=328 width=479)\nSELECT * FROM pg_class where relname = 'schemes';\npg_class has the metadata about the tables.\nCost in the query plan is calculated using the following formula\nCOST = (disk read pages x seq_page_cost) + (rows scanned x cpu_tuple_cost).\nDisk read pages and rows scanned are the properties of pg_class. Seq page cost is the estimated cost of disk read fetch. Cpu tuple cost is the estimated cost of processing each row. eg., COST (54 x 1.0) + (1000 x .01) = 64 Let\u0026rsquo;s see how the query plan changes when we apply filters in the select statement. EXPLAIN SELECT * FROM schemes WHERE status = 'active';\nQUERY PLAN | Seq Scan on schemes (cost=0.00..66.50 rows=960 width=384) Filter: ((status)::text = \u0026lsquo;active\u0026rsquo;::text)\nThe estimated cost here is higher than the previous query, this is because Postgres is performing a seq scan over 1000 rows first and then filtering out the rows based on the WHERE clause.\nPlanner cost constants The cost variables described in this section are measured on an arbitrary scale. Only their relative values matter, hence scaling them all up or down by the same factor will result in no change in the planner\u0026rsquo;s choices. By default, these cost variables are based on the cost of sequential page fetches; that is, seq_page_cost is conventionally set to 1.0 and the other cost variables are set with reference to that. But you can use a different scale if you prefer, such as actual execution times in milliseconds on a particular machine.\nUnfortunately, there is no well-defined method for determining ideal values for the cost variables. They are best treated as averages over the entire mix of queries that a particular installation will receive. This means that changing them on the basis of just a few experiments is very risky. Refer planner cost constants].\nEXPLAIN SELECT schemes.rules FROM scheme_rules JOIN schemes ON (scheme_rules.scheme_id = schemes.id ) where scheme_name = 'weekend_scheme'; The query planner sometimes decides to use a two-step plan. The reason for using two-plan node is the first plan sorts the row locations identified by the index into physical order before reading them, and the other plan actually fetches those rows from the table.\nDown to the nuts and bolts The first plan which does the sorting is called Bitmap scan.\nMost common occurring matches are scanned using the seq scan and the least common matches are scanned using index scan, anything in between is scanned using bitmap heap scan followed by an index scan. One of the reasons for this is that the random I/O is very slow as compared to sequential I/O. This is all driven by analysing statistics.\nBitmap heap scan A bitmap heap scan is like a seq scan - the difference being, rather than visiting every disk page, it scans ANDs and ORs of the applicable index together and only visits the disk pages it needs to visit. This is different from index scan where the index is visited row by row in order, which results in disk pages being visited multiple times. A bitmap scan will sequentially open a short list of disk pages and grab every applicable row in each one.\nSequential scan vs index scan There are cases where a sequential scan is faster than an index scan. When reading data from a disk, a sequential method is usually faster than reading in random order. This is because an index scan requires several I/O for each row which includes looking up a row in the index, and based on that, looking up and retrieving the row from memory(heap). On the other hand, a sequential scan requires a single I/O operation to retrieve more than one block containing multiple rows.\nQuery plan on JOINS The optimizer needs to pick the correct join algorithm when there are multiple tables to be joined in the select statement. Postgres uses three different kinds of join algorithms based on the type of join we are using.\nLet\u0026rsquo;s dive in\nNested Loop: Here the planner can use either a sequential scan or index scan for each of the elements in the first table. The planner uses a sequential scan when the second table is small. The basic logic of choosing between a sequential scan and index scan applies here too. Hash Join: In this algorithm, the planner creates a hash table of the smaller table on the join key. The larger table is then scanned, searching the hash table for the rows which meet the join condition. This requires a lot of memory to store the hash table in the first place. Merge Join: This is similar to merge sort algorithm. Here the planner sorts both the tables to be joined on the join attribute. The tables are then scanned in parallel to find the matching values.\nFurther reading on how we can use Explain/Analyze with join statements here\nPostgres performance tuning is a complicated task. The complexity comes in identifying the appropriate \u0026rsquo;tunable\u0026rsquo; from the many tools that Postgres provides. As you might have now guessed, there is no silver bullet to solving performance issues in Postgres - it\u0026rsquo;s the use case that dictates the tuning requirements. 😅\nHope this helped. Keep following the blog for more updates! 🙂\n","permalink":"http://kshnsink.com/posts/postgres-performance-tuning-manualquery-plans/","summary":"Postgres is one of the most widely used open source databases in the world. At GOJEK, a lot of our products depend on Postgres as well. A lot of major companies use Postgres as their main database however when you\u0026rsquo;re building and operating at scale, the volume of data passing through the pipelines can slow down the most efficient systems.\nTo optimise things in order to enhace performace, we can target the brain of the database whhich is optimizer.","title":"Postgres Performance Tuning Manual: Query Plans"},{"content":"A VPC native cluster uses three unique subnet ranges to allocate IPs to Nodes, Pods and Services.\nPrimary subnet IP address is used for Nodes. Node IP provides connectivity from control components like kube-proxy and kubelet to the Kubernetes API server. Node IP is the node’s connection to the rest of the cluster.\nSecondary subnet IP address is used for Pods. Pod IP addresses are natively routable within the cluster’s VPC network and other VPC networks connected to it by VPC Network Peering. By default GKE allocates /24 alias ie., 256 alias IP addresses for 110 pods for each of the nodes.\nAnother Secondary subnet IP address is used for services. Each Service has an IP address, called the ClusterIP, assigned from the cluster’s VPC network.\nIn a VPC native cluster, these addresses are reserved before the creation of cluster to eliminate conflict and overlapping of IPs.\nWhat happens when the secondary IP range exhausts?\nOnce the secondary IP address exhausts no Pods can be scheduled. The secondary Pod IP range cannot be changed once created. There is a provision to allocate a separate IP range to create separate node pools in the cluster. These two IP ranges are going to be discontigous and there are some caveats which needs to be taken care of.\nIf you use ip-masq-agent configured with the nonMasqueradeCIDRs parameter, you must update the nonMasqueradeCIDRs to include all Pod CIDR ranges.\nIf you use NetworkPolicy configured with ipBlock to specify traffic, you must update the cidr value to include all Pod CIDR ranges.\nThe other approach is to create a bigger CIDR range node pool and move workloads from the existing node pool to the newly created one. Steps for the same:\nMark the nodes in the existing node pool to be non schedulable by using the kubernetes cordon command\nkubectl cordon \u0026lt;node-name\u0026gt;\nRedeploy the application and verify if the nodes are scheduling in the nodes of new node pool\nOnce verified, redeploy and move all the workloads in the new node pool.\nClean up and delete the old node pool.\n","permalink":"http://kshnsink.com/posts/extend-cidr/","summary":"A VPC native cluster uses three unique subnet ranges to allocate IPs to Nodes, Pods and Services.\nPrimary subnet IP address is used for Nodes. Node IP provides connectivity from control components like kube-proxy and kubelet to the Kubernetes API server. Node IP is the node’s connection to the rest of the cluster.\nSecondary subnet IP address is used for Pods. Pod IP addresses are natively routable within the cluster’s VPC network and other VPC networks connected to it by VPC Network Peering.","title":"Extend CIDR"}]