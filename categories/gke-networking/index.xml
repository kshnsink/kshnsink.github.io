<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GKE-Networking on Kishan&#39;s World</title>
    <link>http://kishansharma.in/categories/gke-networking/</link>
    <description>Recent content in GKE-Networking on Kishan&#39;s World</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Mar 2023 01:17:57 +0530</lastBuildDate><atom:link href="http://kishansharma.in/categories/gke-networking/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Extend CIDR</title>
      <link>http://kishansharma.in/posts/extend-cidr/</link>
      <pubDate>Thu, 23 Mar 2023 01:17:57 +0530</pubDate>
      
      <guid>http://kishansharma.in/posts/extend-cidr/</guid>
      <description>Guide to extend CIDR Range in the existing GKE cluster</description>
      <content:encoded><![CDATA[<p>A <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips">VPC native cluster</a> uses three unique subnet ranges to allocate IPs to Nodes, Pods and Services.</p>
<p>Primary subnet IP address is used for Nodes. Node IP provides connectivity from control components like kube-proxy and kubelet to the Kubernetes API server. Node IP is the node’s connection to the rest of the cluster.</p>
<p>Secondary subnet IP address is used for Pods. Pod IP addresses are natively routable within the cluster’s VPC network and other VPC networks connected to it by VPC Network Peering. By default GKE allocates /24 alias ie., 256 alias IP addresses for 110 pods for each of the nodes.</p>
<p>Another Secondary subnet IP address is used for services. Each Service has an IP address, called the ClusterIP, assigned from the cluster’s VPC network.</p>
<p>In a VPC native cluster, these addresses are reserved before the creation of cluster to eliminate conflict and overlapping of IPs.</p>
<p>What happens when the secondary IP range exhausts?</p>
<p>Once the secondary IP address exhausts no Pods can be scheduled. The secondary Pod IP range cannot be changed once created. There is a provision to allocate a separate IP range to create separate node pools in the cluster. These two IP ranges are going to be discontigous and there are some caveats which needs to be taken care of.</p>
<p>If you use ip-masq-agent configured with the nonMasqueradeCIDRs parameter, you must update the nonMasqueradeCIDRs to include all Pod CIDR ranges.</p>
<p>If you use NetworkPolicy configured with ipBlock to specify traffic, you must update the cidr value to include all Pod CIDR ranges.</p>
<p>The other approach is to create a bigger CIDR range node pool and move workloads from the existing node pool to the newly created one. Steps for the same:</p>
<ol>
<li>
<p>Mark the nodes in the existing node pool to be non schedulable by using the kubernetes cordon command</p>
<p><code>kubectl cordon &lt;node-name&gt;</code></p>
</li>
<li>
<p>Redeploy the application and verify if the nodes are scheduling in the nodes of new node pool</p>
</li>
<li>
<p>Once verified, redeploy and move all the workloads in the new node pool.</p>
</li>
<li>
<p>Clean up and delete the old node pool.</p>
</li>
</ol>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
