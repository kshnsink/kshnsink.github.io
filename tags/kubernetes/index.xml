<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kubernetes on Kishan&#39;s World</title>
    <link>http://kshnsink.com/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Kishan&#39;s World</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Jun 2023 02:19:35 +0530</lastBuildDate><atom:link href="http://kshnsink.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Resource Mangement in Kubernetes Cluster: Sizing a Kubernetes Cluster</title>
      <link>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clustersizing-a-kubernetes-cluster/</link>
      <pubDate>Sun, 18 Jun 2023 02:19:35 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clustersizing-a-kubernetes-cluster/</guid>
      <description>Sizing a Kubernetes cluster with an example</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>When sizing a Google Kubernetes Engine (GKE) cluster, several factors come into play such as the expected workload, resource requirements, performance needs etc. Similar to performance, scalability is a very critical factor to the success of a growing organisation which should be considered during the design of a cluster.</p>
<p>Scalability of a Kubernetes cluster is not one dimensional but a combination of a lot of components. When considering the scalability of a Kubernetes cluster, a better way to think about scalability is that within all possible configurations of the cluster there is a cube which wraps an envelope of those configurations which will offer you good stability and  performance and as long as you are within that envelope essentially meaning within the limits of scalability on multiple dimensions you are safe and you can say that you are within the scalability limits.</p>
<p><img loading="lazy" src="/img/kubernetes_scalability_envelope.jpg#center" alt="Scalability envelope"  />
</p>
<p>However, it&rsquo;s important to note that the dimensions that describe the configurations of the cube are not independent. This means that moving further in any one dimension may contract the viable space in the other dimensions. In practical terms, this means that increasing scalability in one area may come at the cost of reduced scalability in another area.</p>
<p>One potential solution to this issue is to decompose the cube into independent sub-cubes. This approach can help reduce the dimensionality of the problem and make it easier to focus on specific areas of scalability. By breaking down the cube into smaller, more manageable parts, it is possible to more effectively analyze and optimize each individual component.</p>
<h2 id="defining-a-kubernetes-cluster">Defining a Kubernetes cluster</h2>
<p>The Kubernetes cluster can scale within a range and limits to accommodate workloads and resource demands effectively. Kubernetes scalability envelope defines the boundaries and constraints of scalability based on various factors such as cluster size, node capacity, resource allocation and performance requirements. The scalability envelope helps cluster administrators and operators understand the cluster‚Äôs capabilities and plan for scaling appropriately.</p>
<p>The key considerations during designing a cluster include:</p>
<ol>
<li>Cluster Size: Sizing a cluster correctly to accommodate all the workloads is very important. Resource is not cheap, if we oversize a cluster, we waste resources ü§¶ and if we undersize the cluster, we get production issues üòõ. Sizing basically takes into account the desired number of worker nodes in the cluster. It defines the minimum and maximum limits for the number of nodes that can be added or removed from the cluster to handle workload fluctuations efficiently.</li>
<li>Node Capacity: Each worker node in the cluster has its own capacity in terms of CPU, memory, and other resources. Kubernetes can work with a wide range of node sizes. Choose appropriate worker node instance types based on CPU, memory, and other resource specifications. Using the most cost effective node size that the cloud provider offers is considered an optimal solution but often times larger nodes works out to be cheaper. This increases pod density and the pods are not scattered out in the cluster which minimizes the idea of pod replication and high availability in case of a node failure. In these cases try adding some smaller nodes too to help with redundancy.</li>
<li>Resource Allocation: The scalability envelope includes resource allocation considerations such as CPU, memory, storage, and network bandwidth. It defines the minimum and maximum limits for resource allocation per pod, ensuring that the cluster can scale within those limits without overcommitting or under-utilizing resources.</li>
<li>High Availability: Ensure sufficient redundancy and fault tolerance by sizing your cluster with multiple worker nodes. This helps distribute the workload and prevent a single point of failure.</li>
<li>Future Growth: Consider future growth and scalability needs of your application or addition of new applications. Plan for potential increases in workload, additional features, or user base expansion. Ensure that the cluster sizing accounts for these future requirements.</li>
</ol>
<h2 id="kubernetes-scalability-thresholds">Kubernetes Scalability Thresholds</h2>
<p>Quoting Kubernetes official community <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#how-we-define-scalability">documentation</a>, scalability thresholds is defined on the basis of ‚ÄúYou Promise, We Promise‚Äù principle</p>
<p><strong>If you promise to:</strong></p>
<ul>
<li>correctly configure your cluster</li>
<li>use extensibility features &ldquo;reasonably&rdquo;</li>
<li>keep the load in the cluster within recommended limits</li>
</ul>
<p><strong>then we promise that your cluster scales, i.e.:</strong></p>
<ul>
<li>all the SLOs are satisfied</li>
</ul>
<p>More on Kubernetes scalability thresholds <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">here</a> and <a href="https://docs.google.com/presentation/d/1aWjxpY4YJ4KJQUTqaVHdR4sbhwqDiW30EF4_hGCc-gI/edit#slide=id.g499cd019d8_0_3">here</a></p>
<blockquote>
<p>Sizing a real world production kubernetes cluster üëá below. This calculation can be taken as a reference and modified as per the requirements.</p>
</blockquote>
<h2 id="sizing-a-kubernetes-cluster-an-example">Sizing a Kubernetes Cluster, an example</h2>
<p><em>For reference, we are using GKE as our cloud provider.</em></p>
<h3 id="goal">Goal</h3>
<p>The goal is to define the configurations for a Kubernetes Cluster.</p>
<h3 id="calculating-total-capacity">Calculating Total Capacity</h3>
<p>Considering the total requirement of CPU and Memory in the Cluster is <code>90</code> vCPU and <code>250</code> GBs. Total number of replicas(pods) in the cluster is <code>40</code>. Mean requirement of a pod is <code>2</code> vCPU and <code>4</code> GB.</p>
<h3 id="node-size-number-of-nodes">Node size (number of nodes)</h3>
<ul>
<li><strong>Decision</strong>:  n2-standard-8</li>
<li><strong>Rationale</strong>: We do not want to go with a bigger node size to avoid the larger blast radius in case of a single node failure.</li>
</ul>
<h3 id="max-nodes-in-the-cluster">Max nodes in the Cluster</h3>
<ul>
<li><strong>Decision</strong>: 32 total, 28 usable</li>
<li><strong>Rationale</strong>: For a max CPU limit in the existing cluster of <code>90</code> vCPU and memory of <code>250</code> GB, 12 nodes <code>n2-standard-8</code> will have 96(=12*8) CPU. Keeping twice the capacity of the required resources as a buffer, we would need a total of 24 nodes. As per GKE <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing_primary_range">recommendations</a>, we need to use /27 which results in 28 (4 nodes are reserved by GKE ) nodes cluster. The first two and last two IP addresses of a primary IP address range are reserved by Google Cloud.</li>
</ul>
<h3 id="podsnode">Pods/node</h3>
<ul>
<li><strong>Decision</strong>: 32</li>
<li><strong>Rationale:</strong> As per GKE autopilot recommendations, the autopilot clusters can have a maximum of <code>32</code> Pods per node. As with the GKE standard, this results in a <code>/26</code> range being provisioned per node which is <code>64</code> IPs.</li>
</ul>
<h3 id="cluster-cidrs">Cluster CIDRs</h3>
<ol>
<li>
<p><strong>Primary subnet IP range</strong></p>
<ul>
<li><strong>Decision</strong>:  - <code>/26</code></li>
<li><strong>Rationale</strong>: The Primary range is used to assign IPs to nodes in the cluster. With a max of 32 nodes in the cluster, we would need a <code>/27</code> range. Doubling this range to <code>/26</code> to support IP exhaustion cases during cluster upgrades or node pool drain.</li>
</ul>
</li>
<li>
<p><strong>Secondary subnet IP range for pods</strong></p>
<ul>
<li><strong>Decision</strong>:  - <code>/22</code></li>
<li><strong>Rationale</strong>: The secondary subnet IP is used to assign pods in the cluster. With a maximum of <code>32</code> pods in a node, we will need a <code>/26</code> IP range for each node which gives us a total requirement for a <code>32</code> nodes cluster of <code>/22</code> (32*32 = 1024, /22 = 1024)</li>
</ul>
</li>
<li>
<p><strong>Secondary subnet IP range for services</strong></p>
<ul>
<li><strong>Decision</strong>:  - <code>/25</code></li>
<li><strong>Rationale</strong>: Considering the number of applications getting deployed in this cluster is <code>26</code>. Keeping a buffer with the introduction of a few new services we can use <code>/25</code> which is <code>128</code> IPs.</li>
</ul>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>Sizing a Kubernetes cluster is nothing less than an art. Properly sizing a Kubernetes cluster is a crucial step in ensuring optimal performance, resource utilization, and scalability for your applications.  I‚Äôve tried to cover the basics here but by carefully considering factors such as HPA, VPA, Quotas and LimitRanges, workload scalability, node capacity, high availability, performance objectives, storage needs, networking considerations, scaling policies, and future growth, you can design a well-sized cluster that meets your application&rsquo;s demands.</p>
<p>Proper cluster sizing involves estimating CPU and memory requirements, selecting suitable worker node instance types, and ensuring redundancy and fault tolerance through multiple nodes. It also requires setting thresholds and policies for scaling based on resource utilization and workload demand, as well as implementing monitoring and observability to continuously monitor the cluster&rsquo;s performance.</p>
<p>Remember that these sizing guidelines provide a starting point, and the specific requirements may vary based on your workload and application characteristics. Regular monitoring and adjustments are necessary to optimize resource allocation, accommodate growth, and maintain optimal performance.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Resource Management in Kubernetes Cluster: Isolation Using Namespaces</title>
      <link>http://kshnsink.com/posts/resource-management-in-kubernetes-clusterisolation-using-namespaces/</link>
      <pubDate>Mon, 29 May 2023 09:55:51 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/resource-management-in-kubernetes-clusterisolation-using-namespaces/</guid>
      <description>Understanding Isolation using Namespaces in Kubernetes</description>
      <content:encoded><![CDATA[<h1 id="isolation-using-namespaces">Isolation using Namespaces</h1>
<p>In Kubernetes, namespaces are a way to create virtual clusters within a physical cluster, providing resource isolation and segregation. Think of it like kernel namespace which is a feature to isolate resources from each other. Analogous to a physical world example, think of it like a store in a shopping mall which has its own physical piece of land, electricity bill, water bill, interior design in a shared shopping complex. The shopping complex is our cluster and the store is a namespace within the cluster. A namespace is a logical boundary that allows you to divide cluster resources and create separate environments for different applications, teams, or projects.</p>
<h2 id="how-namespaces-create-isolation">How Namespaces create Isolation?</h2>
<p>Resource isolation using namespaces helps prevent interference between different workloads running on the same Kubernetes cluster. Let‚Äôs see how it can be achieved:</p>
<h3 id="namespaced-scope">Namespaced Scope</h3>
<p>Kubernetes resources such as pods, services, deployments, replica sets belong to a specific namespace. By default, resources are created in the <code>default</code> namespace if no namespace is specified. Each namespace provides an isolated environments where resources can be created and managed. This means that you could have a service called <code>HelloWorld</code> in a <code>production</code> namespace and a different service called <code>HelloWorld</code> in the <code>test</code> namespace and there won‚Äôt be any conflict.</p>
<blockquote>
<p>Namespaces are logically isolated from one another but they can still communicate with services in another namespace.</p>
</blockquote>
<p>Service DNS names folllow the <code>SERVICE.NAMESPACE.svc.cluster.local</code> pattern. Kubernetes DNS service directory can easily locate any service by its name by using the expanded form of DNS addressing. To access HelloWorld service in the production namespace, simply use <code>helloworld.production</code></p>
<h3 id="resource-allocation">Resource Allocation</h3>
<p>Namespaces allow you to allocate and limit resources separately for different workloads. For example, you can allocate specific amount of CPU and memory resources to a namespace and the resources within the namespace will be constrained by the specified limits. This is done with the help of ResourceQuotas and LimitRanges which are the objects used to control resource usage by the cluster administrator.</p>
<h4 id="resourcequota">ResourceQuota</h4>
<p>The ResourceQuota is the total allocated resources for a particular namespace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ResourceQuota</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">quota</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">hard</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">limits.cpu</span>: <span style="color:#e6db74">&#34;50&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">limits.memory</span>: <span style="color:#ae81ff">100Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests.cpu</span>: <span style="color:#e6db74">&#34;25&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests.memory</span>: <span style="color:#ae81ff">50Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">pods</span>: <span style="color:#e6db74">&#34;50&#34;</span>
</span></span></code></pre></div><h4 id="limitrange">LimitRange</h4>
<p>LimitRange is used for managing constraints of resource allocation at a pod and container level within the namespace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;v1&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#e6db74">&#34;LimitRange&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;resource-limits&#34;</span> 
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>    -
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;Pod&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">max</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;2&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;1Gi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">min</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;200m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;6Mi&#34;</span> 
</span></span><span style="display:flex;"><span>    -
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;Container&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">max</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;2&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;1Gi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">min</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;100m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;4Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">default</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;300m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;200Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">defaultRequest</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;200m&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;100Mi&#34;</span> 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">maxLimitRequestRatio</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;10&#34;</span>
</span></span></code></pre></div><p><img loading="lazy" src="/img/kubernetes_resource_quota_limit_range.png" alt=" "  />
</p>
<h3 id="access-control">Access Control</h3>
<p>Namespaces provide a mechanism for access control and permissions. You can define role-based access control(<strong>RBAC</strong>) policies to grant different levels of access to users or groups for resources within a namespace. This enables fine-grained control over who can access and modify(update) resources within a specific cluster and also within a namespace, enhancing security and isolation. In RBAC there are three components</p>
<h4 id="users-and-groups">Users and Groups</h4>
<p>Users are individuals who need access to the cluster, while groups are collections of users with similar roles</p>
<h4 id="roles">Roles</h4>
<p>A role is a set of rules that define a set of permissions within a specific namespace. Roles can be created at the namespace level using <code>Role</code>object and are used to grant access to resources within the namespace. It is also possible to define the role which has access across the cluster using <code>ClusterRole</code> object</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>] <span style="color:#75715e"># &#34;&#34; indicates the core API group</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;pods&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># &#34;namespace&#34; omitted since ClusterRoles are not namespaced</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># at the HTTP level, the name of the resource for accessing Secret</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># objects is &#34;secrets&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;secrets&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span></code></pre></div><h4 id="rolebindings-and-clusterrolebindings">RoleBindings and ClusterRoleBindings</h4>
<p><strong>RoleBindings</strong> associate a role with a user or a group, granting the defined permissions to the user/group within a specific namespace.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This role binding allows &#34;jane&#34; to read pods in the &#34;default&#34; namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You need to already have a Role named &#34;pod-reader&#34; in that namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-pods</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You can specify more than one &#34;subject&#34;</span>
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">jane</span> <span style="color:#75715e"># &#34;name&#34; is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># &#34;roleRef&#34; specifies the binding to a Role / ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span> <span style="color:#75715e">#this must be Role or ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span> <span style="color:#75715e"># this must match the name of the Role or ClusterRole you wish to bind to</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p><strong>ClusterRoleBindings</strong> on the other hand, associate a role with a user/group across the entire cluster, granting permissions across all namespaces</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This role binding allows &#34;dave&#34; to read secrets in the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You need to already have a ClusterRole named &#34;secret-reader&#34;.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-secrets</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># The namespace of the RoleBinding determines where the permissions are granted.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># This only grants permissions within the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">development</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dave</span> <span style="color:#75715e"># Name is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p>Implementing RBAC enforces strong access controls ensuring that only authorized users/groups have appropriate level of access to cluster resources.</p>
<p>By leveraging namespaces, Kubernetes provides a powerful mechanism for isolating and managing resources within a cluster. It allows you to run multiple applications or projects on the same cluster without interference or noisy neighbour problem and provides control over resource allocation, access control, and resource quotas at the namespace level.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Resource Management in Kubernetes Cluster: Understanding Resources</title>
      <link>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterunderstanding-resources/</link>
      <pubDate>Thu, 11 May 2023 03:52:06 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/resource-mangement-in-kubernetes-clusterunderstanding-resources/</guid>
      <description>Understanding Kubernetes resources and Quality of Service classes</description>
      <content:encoded><![CDATA[<p>Kubernetes is a container orchestration platform that allows you to manage and automate the deployment, scaling and management of containerised application. In this post we will look into the resource management in Kubernetes which involves allocating resources such as CPU, memory to the containers running in a cluster</p>
<h1 id="understanding-resource">Understanding Resource</h1>
<p>At the heart of a Kubernetes cluster  is a resource abstraction called <strong>pods</strong> which groups containers together and manage their resources as a unit. Each pod is assigned a certain amount of CPU and memory resources, which are used by the containers running within the pod. Pods run on a node in the cluster and the node size in terms of memory and CPU in a cluster is definite. Only certain number of pods can be run in a cluster and it is a duty of a Kubernetes cluster admin to assign the resources to the pod optimally to get the best utilization and at the same time ensure that there‚Äôs enough room to deal with increasing load and failures.</p>
<p>Kubernetes scheduler decides where to place a pod in the cluster, basically which node has spare resource. In order to schedule pods effectively, the scheduler must know the resource requirements for each pod. This is where Kubernetes resource requests and limits kicks in</p>
<h2 id="container-resource">Container Resource</h2>
<h3 id="requests-and-limitshttpskubernetesiodocsconceptsconfigurationmanage-resources-containersrequests-and-limits"><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Requests and Limits</a></h3>
<p>Kubernetes resource configuration consists of two components: <code>requests</code> and <code>limits</code>.</p>
<ul>
<li>request specifies the minimum amount of a request that a pod needs to run.</li>
<li>limit specifies the maximum amount of resource that a pod is allowed to use.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;200Mi&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;400m&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;400Mi&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;800m&#34;</span>
</span></span></code></pre></div><p>Setting resource requests and limits allows to accommodate spiky Pods in a cluster. Resource limits are a hard boundary for a pod. A pod that tries to use more than its allocated CPU limit which is a <em>compressible resource</em> will be throttled thus impacting performance and if it tries to use more than the allowed memory limit which is an incompressible resource, the pod gets terminated with OOM error. Scheduler tries to schedule the pod in any other nodes in the cluster if there‚Äôs enough memory in the node. Kubernetes allows resources to be overcommitted which means the sum of all the resource limits of containers on a cluster(node) can exceed the total resources.</p>
<p>Despite all the efforts put upfront in defining the resource requirements, what happens if the containers are using more resources than allocated. Kubernetes uses <code>Quality of Service (QoS)</code> classes to make a decision about evicting pods in resource crunch situations.</p>
<h2 id="quality-of-serviceqos">Quality of Service(QoS)</h2>
<p>Kubernetes has 3 QoS classes <em>Guaranteed</em>, <em>Burstable</em>, or <em>BestEffort</em> defined. This cannot be assigned directly by you, rather Kubernetes does it for you on the basis of resources defined in the Pod manifest.</p>
<h3 id="guaranteed">Guaranteed</h3>
<p>When containers Limits(CPU and Memory) matches requests(CPU and memory). This essentially means that control plane kills this Pod if it exceeds the specified limits.</p>
<p><img loading="lazy" src="/img/guaranteed_qos_class.jpg#center" alt="guaranteed"  />
</p>
<blockquote>
<p>Note:¬†If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">kubectl get po qos-pod-guaranteed -o yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">status</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">qosClass</span>: <span style="color:#ae81ff">Guaranteed </span>
</span></span></code></pre></div><h3 id="burstable">Burstable</h3>
<p>When containers Limits is higher than Requests. Kubernetes allows burstable pods upto their limit if capacity is available in the node. If the pod uses more resources than the request and there‚Äôs not enough resource available in the node, the pod gets terminated.</p>
<p><img loading="lazy" src="/img/burstable_qos_class.jpg#center" alt="burstable"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">kubectl get po qos-pod-burstable -o yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">200m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">400Mi</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">400m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">800Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">...</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">status</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">qosClass</span>: <span style="color:#ae81ff">Burstable </span>
</span></span></code></pre></div><h3 id="besteffort">BestEffort</h3>
<p>When containers doesn‚Äôt specify any resource request or limits. In this situation pods are allowed to use whatever resource is available on the node but it will be the first one to be killed when the cluster needs to make room for higher(Burstable/Guaranteed) QoS pods.</p>
<p><img loading="lazy" src="/img/besteffort_qos_class.jpg#center" alt="best-effort"  />
</p>
<h2 id="what-should-you-do">What should you do?</h2>
<p>In the first instance it seems that Guaranteed is the best QoS class to set for the pods. But remember resource(memory/CPU) has some cost üí∞. Let us try and understand how this can be approached. Considering we have a definite resource at hand and if we want to set request and limits equal(Guaranteed QoS class) we have to either increase requests or lower the limits. In case of increasing the resource, we might be blocking too much of resource unnecessarily and some other Pod which could have been scheduled in the node is not getting the resource. If we lower the Limit, the pod might throttle during the peak/spike hours. This seems like a similar territory when VMs are used.</p>
<blockquote>
<p><em>Scheduling of a Pod is based on requests and not limits</em></p>
</blockquote>
<p>The place where Kubernetes is different from the conventional resource allocation is when we use Burstable pods. Here, the amount of resource blocked by the pod is lower than the amount of resource pod needs during surge hours.</p>
<p>Pods with QoS class as <em>BestEffort</em> gets the <strong>lowest priority.</strong> If resource is not specified, Kubernetes scheduler will place such pods on any of the node which has available resource.</p>
<p>During the eviction, Kubelet selects the Pods to evict in order of QoS class. Pods classified as BestEffort will be first evicted followed by Burstable and finally Guaranteed.</p>
<blockquote>
<p><em>Always specify resource requests and limits. This helps Kubernetes schedule and manage the pods properly. For critical pods or stateful sets, prefer Guaranteed class and Burstable for the less critical ones</em></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Prometheus Monitoring</title>
      <link>http://kshnsink.com/posts/kubernetes-prometheus-monitoring/</link>
      <pubDate>Tue, 25 Apr 2023 01:26:36 +0530</pubDate>
      
      <guid>http://kshnsink.com/posts/kubernetes-prometheus-monitoring/</guid>
      <description>Using Prometheus to monitor kubernetes cluster</description>
      <content:encoded><![CDATA[<h2 id="monitoring-kubernetes-cluster-with-prometheus">Monitoring Kubernetes cluster with Prometheus</h2>
<p>Prometheus is a widely used open-source monitoring system that is commonly used for monitoring Kubernetes environments. In Kubernetes, Prometheus can be used to monitor various Kubernetes components such as pods, nodes, and services. Kubernetes provides an API that allows Prometheus to discover the endpoints of the different components and collect metrics from them. These metrics can include CPU usage, memory usage, network traffic, and other relevant information. Prometheus also provides a variety of built-in visualization tools such as Grafana, which can be used to visualize the collected metrics. This allows users to create dashboards that provide a high-level view of the cluster&rsquo;s health and performance.</p>
<p>There are fundamentally two things that we can monitor in kubernetes system</p>
<ul>
<li>Monitor applications running on kubernetes infrastructure</li>
<li>Monitor kubernetes cluster
<ul>
<li>control plane components such as coreDNS, apiserver, kube scheduler</li>
<li>kubelet(cAdvisor) which exposes container metrics</li>
<li>kube-state-metrics which is basically the cluster level metrics around deployments, pod etc</li>
<li>node-exporter which runs on all the nodes and exposes metrics around CPU, memory, network. Node exporter can be run on a kubernetes cluster in the following ways
<ul>
<li>manually run in each nodes in the cluster</li>
<li>use kubernetes daemonset which allows to run pod of node-exporter in all the nodes in the cluster</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Kubernetes doesn‚Äôt expose these metrics by default. For that we need to install kube state metrics container into our kubernetes environment and this container is responsible for making it available to the prometheus server.</p>
<h2 id="deploying-prometheus">Deploying Prometheus</h2>
<p>There are multiple options to deploy prometheus.</p>
<ol>
<li>Manually deploy premotheus on kubernetes. this requires manually creating all the deployments, configmaps, services secrets etc.</li>
<li>Deploy using Helm chart to deploy prometheus operator</li>
</ol>
<h2 id="operators-in-kubernetes">Operators in Kubernetes</h2>
<p>A kubernetes operator is a method of packaging, deploying and managing a kubernetes application. A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user.</p>
<h2 id="prometheus-operatorhttpsgithubcomprometheus-operatorprometheus-operator"><a href="https://github.com/prometheus-operator/prometheus-operator">Prometheus operator</a></h2>
<p>The Prometheus Operator provides Kubernetes native deployment and management of Prometheus and related monitoring components.
The Prometheus operator includes the following features:</p>
<ul>
<li><strong>Kubernetes Custom Resources</strong>: Use Kubernetes custom resources to deploy and manage Prometheus, AlertManager, and related components.</li>
<li><strong>Simplified Deployment Configuration</strong>: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource.</li>
<li><strong>Prometheus Target Configuration</strong>: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.</li>
</ul>
<p><a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md">user guide</a></p>
<p>This operator comes with several resources such as AlertManager, ServiceMonitor, PodMonitor, PrometheusRule, AlertManager config</p>
<h2 id="service-monitors">Service Monitors</h2>
<p>The Prometheus operator comes with several custom resource definitions that provide a high level abstraction for deploying prometheus.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get crd
</span></span><span style="display:flex;"><span>servicemonitors.monitoring.coreos.com       2023-04-24T12:28:54Z
</span></span><span style="display:flex;"><span>prometheusrules.monitoring.coreos.com       2023-04-24T12:28:54Z
</span></span></code></pre></div><p><strong>Service monitors</strong> define a set of targets for prometheus to monitor and scrape. They allow you to avoid touching prometheus configs directly and give you a declarative kubernetes syntax to define targets</p>
<blockquote>
<p>Writing and maintaining configuration in prometheus is a pain, that‚Äôs why there‚Äôs a thing called service monitor. A service monitor tells prometheus what services in kubernetes to monitor so if you have an arbitrary deployment with some pods running behind it and you‚Äôre exposing a service to that pod you can create a service monitor that uses a label selector to select the service and then in prometheus you can label the selectors to select the service monitors that prometheus needs to consume that‚Äôll tell prometheus what service endpoints to start scraping to collect metrics.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">kubectl get servicemonitors.monitoring.coreos.com prometheus-kube-prometheus-prometheus -o yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">monitoring.coreos.com/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceMonitor</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">meta.helm.sh/release-name</span>: <span style="color:#ae81ff">prometheus</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">meta.helm.sh/release-namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#e6db74">&#34;2023-04-24T12:29:25Z&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">generation</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kube-prometheus-stack-prometheus</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">prometheus</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/part-of</span>: <span style="color:#ae81ff">kube-prometheus-stack</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#ae81ff">45.20.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">kube-prometheus-stack-45.20.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">heritage</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">release</span>: <span style="color:#ae81ff">prometheus</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">prometheus-kube-prometheus-prometheus</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resourceVersion</span>: <span style="color:#e6db74">&#34;193503&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">uid</span>: <span style="color:#ae81ff">8be1b353-047e-4b9b-ba15-d1a6517cf2cd</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">endpoints</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/metrics</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">http-web</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespaceSelector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchNames</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kube-prometheus-stack-prometheus</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">release</span>: <span style="color:#ae81ff">prometheus</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">self-monitor</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span></code></pre></div><h2 id="installing-prometheus-with-helm-charthttpsgithubcomprometheus-communityhelm-chartstreemainchartskube-prometheus-stack"><a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack">Installing Prometheus with Helm chart</a></h2>
<p><a href="https://github.com/prometheus-operator/kube-prometheus">kube-prometheus stack</a> is a collection of Kubernetes manifests,¬†<a href="http://grafana.com/">Grafana</a>
dashboards, and¬†<a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">Prometheus rules</a> combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with¬†<a href="https://prometheus.io/">Prometheus</a> using the¬†<a href="https://github.com/prometheus-operator/prometheus-operator">Prometheus Operator</a></p>
<ul>
<li><strong>Get Helm Repository Info</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span style="display:flex;"><span>helm repo update
</span></span></code></pre></div><ul>
<li><strong>Install Helm Chart</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm install prometheus prometheus-community/kube-prometheus-stack
</span></span></code></pre></div><p>This helm chart creates all the prometheus resources in the cluster</p>
<p>To see what it has created, let‚Äôs get all the resources</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get all
</span></span><span style="display:flex;"><span>NAME                                                         READY   STATUS    RESTARTS      AGE
</span></span><span style="display:flex;"><span>pod/alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   <span style="color:#ae81ff">1</span> <span style="color:#f92672">(</span>11m ago<span style="color:#f92672">)</span>   13m
</span></span><span style="display:flex;"><span>pod/prometheus-grafana-6984c5759f-2wmlz                      3/3     Running   <span style="color:#ae81ff">0</span>             14m
</span></span><span style="display:flex;"><span>pod/prometheus-kube-prometheus-operator-5f8db7f79c-j9z9t     1/1     Running   <span style="color:#ae81ff">0</span>             14m
</span></span><span style="display:flex;"><span>pod/prometheus-kube-state-metrics-7fbdd95dc4-nrj49           1/1     Running   <span style="color:#ae81ff">0</span>             14m
</span></span><span style="display:flex;"><span>pod/prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   <span style="color:#ae81ff">0</span>             13m
</span></span><span style="display:flex;"><span>pod/prometheus-prometheus-node-exporter-5bzbv                1/1     Running   <span style="color:#ae81ff">0</span>             14m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>                      AGE
</span></span><span style="display:flex;"><span>service/alertmanager-operated                     ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   13m
</span></span><span style="display:flex;"><span>service/kubernetes                                ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                      13d
</span></span><span style="display:flex;"><span>service/prometheus-grafana                        ClusterIP   10.96.182.45     &lt;none&gt;        80/TCP                       14m
</span></span><span style="display:flex;"><span>service/prometheus-kube-prometheus-alertmanager   ClusterIP   10.111.178.253   &lt;none&gt;        9093/TCP                     14m
</span></span><span style="display:flex;"><span>service/prometheus-kube-prometheus-operator       ClusterIP   10.107.58.215    &lt;none&gt;        443/TCP                      14m
</span></span><span style="display:flex;"><span>service/prometheus-kube-prometheus-prometheus     ClusterIP   10.100.157.20    &lt;none&gt;        9090/TCP                     14m
</span></span><span style="display:flex;"><span>service/prometheus-kube-state-metrics             ClusterIP   10.102.14.155    &lt;none&gt;        8080/TCP                     14m
</span></span><span style="display:flex;"><span>service/prometheus-operated                       ClusterIP   None             &lt;none&gt;        9090/TCP                     13m
</span></span><span style="display:flex;"><span>service/prometheus-prometheus-node-exporter       ClusterIP   10.96.1.108      &lt;none&gt;        9100/TCP                     14m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
</span></span><span style="display:flex;"><span>daemonset.apps/prometheus-prometheus-node-exporter   <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           &lt;none&gt;          14m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-grafana                    1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-kube-prometheus-operator   1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-kube-state-metrics         1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NAME                                                             DESIRED   CURRENT   READY   AGE
</span></span><span style="display:flex;"><span>replicaset.apps/prometheus-grafana-6984c5759f                    <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       14m
</span></span><span style="display:flex;"><span>replicaset.apps/prometheus-kube-prometheus-operator-5f8db7f79c   <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       14m
</span></span><span style="display:flex;"><span>replicaset.apps/prometheus-kube-state-metrics-7fbdd95dc4         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       14m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NAME                                                                    READY   AGE
</span></span><span style="display:flex;"><span>statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager   1/1     13m
</span></span><span style="display:flex;"><span>statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus       1/1     13m
</span></span></code></pre></div><h3 id="resources-created">Resources created</h3>
<blockquote>
<p>Let&rsquo;s understand the important resources we have created</p>
</blockquote>
<h4 id="deployments">Deployments</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get deployments
</span></span><span style="display:flex;"><span>NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-grafana                    1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-kube-prometheus-operator   1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span><span style="display:flex;"><span>deployment.apps/prometheus-kube-state-metrics         1/1     <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           14m
</span></span></code></pre></div><ul>
<li><strong>Prometheus Grafana:</strong> is a graphical UI tool that is used to visualize the data that is there in the prometheus time series database</li>
<li><strong>Kube prometheus operator</strong>: this is the operator that is going to manage the lifecycle of the prometheus instance. It handles the update of configs, restart the process upon changes in the config</li>
<li><strong>Kube state metrics</strong>: container for exposing cluster level metrics such as deployments, pods, services</li>
</ul>
<h4 id="statefulset">StatefulSet</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get statefulset
</span></span><span style="display:flex;"><span>NAME                                                                    READY   AGE
</span></span><span style="display:flex;"><span>statefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager   1/1     13m
</span></span><span style="display:flex;"><span>statefulset.apps/prometheus-prometheus-kube-prometheus-prometheus       1/1     13m
</span></span></code></pre></div><ul>
<li><strong>Prometheus server</strong>: this is a container that‚Äôs running the prometheus process</li>
<li><strong>AlertManager</strong>: alert manager instance</li>
</ul>
<h4 id="daemonset">Daemonset</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get daemonset
</span></span><span style="display:flex;"><span>NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
</span></span><span style="display:flex;"><span>daemonset.apps/prometheus-prometheus-node-exporter   <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           &lt;none&gt;          14m
</span></span></code></pre></div><ul>
<li><strong>Node exporter</strong>: responsible for deploying a node exporter pod on every single node in the cluster and this pod is responsible for collecting host metrics such as CPU utilization, memory utilization and exposes it to prometheus server</li>
</ul>
<h2 id="connecting-to-prometheus-server">Connecting to prometheus server</h2>
<p>The prometheus service is of type clusterIP and can be accessed from within the cluster. To connect to the prometheus server from outside the cluster we can either make the service of type nodeport or load balancer or use an ingress to route traffic to the service.</p>
<p>we can also port forward the prometheus pod to access it locally</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl port-forward prometheus-prometheus-kube-prometheus-prometheus-0 <span style="color:#ae81ff">9090</span>
</span></span></code></pre></div><h2 id="prometheus-kubernetes-configurationhttpsprometheusiodocsprometheuslatestconfigurationconfigurationkubernetes_sd_config"><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">Prometheus kubernetes configuration</a></h2>
<p>Kubernetes SD configurations allow retrieving scrape targets from¬†<a href="https://kubernetes.io/">Kubernetes</a> REST API and always staying synchronized with the cluster state. One of the following role types can be configured to discover targets.</p>
<ul>
<li>role
<ul>
<li>service</li>
<li>node</li>
<li>pod</li>
<li>endpoints</li>
<li>endpointsslice</li>
<li>ingress</li>
</ul>
</li>
</ul>
<blockquote>
<p>The default config uses the role <code>endpoint</code> because the endpoint role discovers targets from listed endpoints of a service and thus we can basically discover pods, services, nodes and everything else using the endpoints</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kubernetes_sd_configs</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">role</span>: <span style="color:#ae81ff">endpoints</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubeconfig_file</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">follow_redirects</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enable_http2</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespaces</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">own_namespace</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">names</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">default</span>
</span></span></code></pre></div><h2 id="prometheus-rules">Prometheus Rules</h2>
<p>To add rules, prometheus operator has a CRD called prometheusrule which handles registering new rules to a prometheus instance</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get prometheusrules.monitoring.coreos.com
</span></span></code></pre></div>]]></content:encoded>
    </item>
    
  </channel>
</rss>
